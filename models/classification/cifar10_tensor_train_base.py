"""
Convolutional Neural Netwok implementation in tensorflow whith fully connected layers.

The neural network is ran against the mnist dataset and we can see an example of distortion of input in the case
where the input comes from memory.
"""

import tensorflow as tf
import time
from pathlib import Path

import keras
from keras.callbacks import LearningRateScheduler, TensorBoard
from keras.preprocessing.image import ImageDataGenerator
from keras.datasets import cifar10
import numpy as np

from tensort_train_baseline_model import tensor_train_baseline

tf.logging.set_verbosity(tf.logging.ERROR)


def scheduler(epoch):
    if epoch < 30:
        return 0.1
    if epoch < 60:
        return 0.01
    return 0.001

# Preparing the dataset and parameters#########################

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

batch_size = 100
num_classes = 10
epochs = 100
iterations = int(x_train.shape[0] / batch_size)
weight_decay = 1e-4

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
mean_x_train = np.mean(x_train, axis=(0, 1, 2))
std_x_train = np.std(x_train, axis=(0, 1, 2))
x_train -= mean_x_train
x_train /= std_x_train
mean_x_test = np.mean(x_test, axis=(0, 1, 2))
std_x_test = np.std(x_test, axis=(0, 1, 2))
x_test -= mean_x_test
x_test /= std_x_test

# Convert class vectors to binary class matrices.
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model_base_name = Path(__file__).stem + "_" + str(int(time.time()))
model = tensor_train_baseline(input_shape=x_train.shape[1:], num_classes=num_classes, weight_decay=weight_decay)

opt = keras.optimizers.SGD(lr=0.001, momentum=0.9)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

print('Using real-time data augmentation.')
# This will do preprocessing and realtime data augmentation:

datagen = ImageDataGenerator(horizontal_flip=True,
                             width_shift_range=0.125,
                             height_shift_range=0.125,
                             fill_mode='constant',
                             cval=0.)

datagen.fit(x_train)

change_lr = LearningRateScheduler(scheduler)

# disabled early stopping because it somehow lead to worse results
# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',
#                               min_delta=0,
#                               patience=3,
#                               verbose=0, mode='auto')

# tb_cb = TensorBoard(log_dir="tb_{}".format(model_base_name), histogram_freq=0)

# Fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(x_train, y_train,
                    batch_size=batch_size),
                    epochs=epochs,
                    steps_per_epoch=iterations,
                    validation_data=(x_test, y_test),
                    # callbacks=[change_lr]
                    )

# Save model and weights
save_dir = Path(__file__).parent / "saved_models"
save_dir.mkdir(exist_ok=True, parents=True)
model_name = model_base_name + ".h5"
model_path = save_dir / model_name
model.save(model_path)
print('Saved trained model at %s ' % model_path)

# Score trained model.
scores = model.evaluate(x_test, y_test, verbose=1)
print('{} Test loss: {}'.format(__file__, scores[0]))
print('{} Test accuracy: {}'.format(__file__, scores[1]))
