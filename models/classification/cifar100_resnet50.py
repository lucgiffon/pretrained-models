from keras.applications.resnet import ResNet50
from keras.applications.resnet_v2 import ResNet50V2
from keras.preprocessing.image import ImageDataGenerator
from keras_contrib.applications.resnet import ResNet18
from keras.datasets import cifar100
import numpy as np
import tensorflow as tf
from pathlib import Path
from keras.callbacks import TensorBoard
import keras
import time


tf.logging.set_verbosity(tf.logging.ERROR)


if __name__ == "__main__":


    # Preparing the dataset and parameters#########################

    (x_train, y_train), (x_test, y_test) = cifar100.load_data()

    batch_size = 128
    num_classes = 100
    epochs = 300
    iterations = int(x_train.shape[0] / batch_size)

    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    mean_x_train = np.mean(x_train, axis=(0, 1, 2))
    std_x_train = np.std(x_train, axis=(0, 1, 2))
    x_train -= mean_x_train
    x_train /= std_x_train
    mean_x_test = np.mean(x_test, axis=(0, 1, 2))
    std_x_test = np.std(x_test, axis=(0, 1, 2))
    x_test -= mean_x_test
    x_test /= std_x_test

    # Convert class vectors to binary class matrices.
    y_train = keras.utils.to_categorical(y_train, num_classes)
    y_test = keras.utils.to_categorical(y_test, num_classes)

    model_base_name = Path(__file__).stem + "_" + str(int(time.time()))

    model = ResNet50(classes=num_classes, include_top=True, weights=None, input_shape=x_train.shape[1:], pooling=None)

    # Add dense layers on top of convolution

    # opt = keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)
    # opt = keras.optimizers.RMSprop(lr=0.0001)
    opt = keras.optimizers.Adam(lr=0.001)

    # Let's train the model using RMSprop
    model.compile(loss='categorical_crossentropy',
                  optimizer=opt,
                  metrics=['accuracy'])

    print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:

    datagen = ImageDataGenerator(horizontal_flip=True,
                                 width_shift_range=0.125,
                                 height_shift_range=0.125,
                                 fill_mode='constant',
                                 cval=0.)

    datagen.fit(x_train)


    # disabled early stopping because it somehow lead to worse results
    # early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',
    #                               min_delta=0,
    #                               patience=3,
    #                               verbose=0, mode='auto')

    tb_cb = TensorBoard(log_dir="tb_{}".format(model_base_name), histogram_freq=0)

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(datagen.flow(x_train, y_train,
                                     batch_size=batch_size),
                        epochs=epochs,
                        steps_per_epoch=iterations,
                        validation_data=(x_test, y_test),
                        callbacks=[tb_cb],
                        verbose=2
                        )

    # Save model and weights
    save_dir = Path(__file__).parent / "saved_models"
    save_dir.mkdir(exist_ok=True, parents=True)
    model_name = model_base_name + ".h5"
    model_path = save_dir / model_name
    model.save(model_path)
    print('Saved trained model at %s ' % model_path)

    # Score trained model.
    scores = model.evaluate(x_test, y_test, verbose=1)
    print('{} Test loss: {}'.format(__file__, scores[0]))
    print('{} Test accuracy: {}'.format(__file__, scores[1]))
