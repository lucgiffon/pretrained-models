"""
Convolutional Neural Netwok implementation in tensorflow whith fully connected layers.

The neural network is ran against the mnist dataset and we can see an example of distortion of input in the case
where the input comes from memory.
"""

import tensorflow as tf
import os
import tempfile
import time
from pathlib import Path

from keras.callbacks import LearningRateScheduler, TensorBoard
import numpy as np
from keras.initializers import he_normal
from keras.preprocessing.image import ImageDataGenerator
from keras.datasets import cifar100
import keras
from keras.models import Model
from conv_vgg19 import VGG19

tf.logging.set_verbosity(tf.logging.ERROR)

import argparse
parser = argparse.ArgumentParser()
parser.add_argument('-s', '--seed', type=int, metavar='NUMBER',
                    help='Seed number. No default.')
parser.add_argument("-m", '--model', default="resnet50", type=str, help="The name of the model to pick from keras applications")
parser.add_argument("-r", '--regularize', default=False, action='store_true', help="Add L2 regularization to layers")
parser.add_argument("-i", '--initialize', default=False, action='store_true', help="Add He Normal initialisation to layers")

args = parser.parse_args()

if args.seed is not None:
    from numpy.random import seed
    seed(args.seed)
    from tensorflow import set_random_seed
    set_random_seed(args.seed)

def scheduler(epoch):
    if epoch < 60:
        return 0.1
    elif epoch < 120:
        return 0.02
    elif epoch < 160:
        return 0.004
    return 0.0008

# Preparing the dataset and parameters#########################

(x_train, y_train), (x_test, y_test) = cifar100.load_data()

batch_size = 128
num_classes = 100
epochs = 300
iterations = int(x_train.shape[0] / batch_size)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
mean_x_train = np.mean(x_train, axis=(0, 1, 2))
std_x_train = np.std(x_train, axis=(0, 1, 2))
x_train -= mean_x_train
x_train /= std_x_train
# mean_x_test = np.mean(x_test, axis=(0, 1, 2))
# std_x_test = np.std(x_test, axis=(0, 1, 2))
x_test -= mean_x_train
x_test /= std_x_train

# Convert class vectors to binary class matrices.
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

# Build model fromm keras vgg19 implementation
str_seed = "" if args.seed is None else f"_{args.seed}"
model_base_name = Path(__file__).stem + "_" + f"{args.model}" + str_seed + "_" + str(int(time.time()))

if args.model == "resnet50":
    model = keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_shape=x_train.shape[1:], classes=num_classes,  pooling=None)
    if args.regularize:
        weight_decay = 5e-4
        for layer in model.layers:
            if hasattr(layer, "kernel_regularizer"):
                layer.kernel_regularizer = keras.regularizers.l2(weight_decay)

    if args.initialize:
        initializer = he_normal
        for layer in model.layers:
            if hasattr(layer, "kernel_initializer"):
                layer.kernel_initializer = initializer()

    with tempfile.TemporaryDirectory() as dirpath:
        model_path = Path(dirpath) / "tmp_model.h5"
        model.save(str(model_path))
        keras.backend.clear_session()
        model = keras.models.load_model(str(model_path))
else:
    raise ValueError(f"Unknown model {args.model}")


# Add dense layers on top of convolution

# initiate RMSprop optimizer
opt = keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)
# opt = keras.optimizers.RMSprop(lr=0.0001)
# opt = keras.optimizers.Adam(lr=0.001)

# Let's train the model using RMSprop
model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

print('Using real-time data augmentation.')
# This will do preprocessing and realtime data augmentation:

datagen = ImageDataGenerator(horizontal_flip=True,
                             rotation_range=15,
                             width_shift_range=0.125,
                             height_shift_range=0.125,
                             fill_mode='constant',
                             cval=0.)

datagen.fit(x_train)

change_lr = LearningRateScheduler(scheduler)

# disabled early stopping because it somehow lead to worse results
# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',
#                               min_delta=0,
#                               patience=3,
#                               verbose=0, mode='auto')

tb_cb = TensorBoard(log_dir="tb_{}".format(model_base_name), histogram_freq=0)

callbacks = [change_lr, tb_cb]
# callbacks = [tb_cb]
# Fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(x_train, y_train,
                    batch_size=batch_size),
                    epochs=epochs,
                    steps_per_epoch=iterations,
                    validation_data=(x_test, y_test),
                    callbacks=callbacks
                    )


# Save model and weights
save_dir = Path(__file__).parent / "saved_models"
save_dir.mkdir(exist_ok=True, parents=True)
model_name = model_base_name + ".h5"
model_path = save_dir / model_name
model.save(model_path)
print('Saved trained model at %s ' % model_path)

# Score trained model.
scores = model.evaluate(x_test, y_test, verbose=1)
print('{} {} Test loss: {}'.format(__file__, args.model, scores[0]))
print('{} {} Test accuracy: {}'.format(__file__, args.model, scores[1]))
